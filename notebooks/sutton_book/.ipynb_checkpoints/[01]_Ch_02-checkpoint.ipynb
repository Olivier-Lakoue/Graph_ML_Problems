{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutli-armed bandit\n",
    "\n",
    "This is a _non-associative_ setting meaning that only one action must be chosen at each timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 A $k$-armed bandit problem\n",
    "Choose one of k possible actions to get the the best value.\n",
    "\n",
    "Action: $A_t$\n",
    "\n",
    "Reward: $R_t$\n",
    "\n",
    "The value of an action $a$ is the expected reward:\n",
    "\n",
    "$$  q_*(a)\\ \\dot=\\ \\Bbb E \\left[R_t\\ \\big|\\ A_t=a\\ \\right]   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the expected reward is not known with certainty, we use the estimated value $Q_t(a)$ instead. Our goal is to get $Q_t(a)$ as close as possible to $q_*(a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Action-value methods\n",
    "\n",
    "* The sample average:\n",
    "\n",
    "The estimate of $Q_t(a)$ is:\n",
    "\n",
    "$$  Q_t(a) = \\frac{\\sum_{i=1}^{t-1} R_i . \\Bbb1_{A_i=a}}{\\sum_{i=1}^{t-1} \\Bbb1_{A_i=a}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fraction of the sum of previous rewards for that action divided by the number of times this action was choosen in the past.\n",
    "\n",
    "$\\Bbb1_{predicate}$ is 1 if $predicate$(ie. $A_i=a$) is true or 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _greedy_ action selection:\n",
    "\n",
    "$$ A_t \\dot= \\ argmax_{a} \\ Q_t(a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercice 2.1 :_\n",
    "\n",
    "With 2 actions and an $\\epsilon=0.5$, what is the probability that the greedy action is selected ?\n",
    "\n",
    "$p=0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercice 2.2:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "q = [[0,0,0,0]]\n",
    "actions = [0,1,1,1,2]\n",
    "rewards = [1,1,2,2,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which timestep a random action was selected ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0]\n",
      "[0, 1, 0, 0]\n",
      "[0, 2, 0, 0]\n",
      "[0, 2, 0, 0]\n",
      "[0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for i,a in enumerate(actions):\n",
    "    q_t = q[0].copy()\n",
    "    q_t[a] = rewards[i]\n",
    "    q.append(q_t)\n",
    "    print(q_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.array(q)\n",
    "q_vals = q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of past rewards\n",
    "for t in range(len(q)):\n",
    "    q_vals[t,:] = np.sum(q[:t,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 1, 0, 0],\n",
       "       [1, 3, 0, 0],\n",
       "       [1, 5, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average by sum of actions\n",
    "q_acts = q.copy()\n",
    "for t in range(len(q)-1):\n",
    "    q_acts[t+1,actions[t]] = q_acts[t,actions[t]] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 2, 0, 0],\n",
       "       [0, 3, 0, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vals = np.divide(q_vals, q_acts, where=(q_acts>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.937e-321, 0.000e+000, 0.000e+000, 0.000e+000],\n",
       "       [0.000e+000, 0.000e+000, 0.000e+000, 0.000e+000],\n",
       "       [1.000e+000, 0.000e+000, 0.000e+000, 0.000e+000],\n",
       "       [1.000e+000, 5.000e-001, 0.000e+000, 0.000e+000],\n",
       "       [1.000e+000, 1.000e+000, 0.000e+000, 0.000e+000],\n",
       "       [1.000e+000, 5.000e+000, 0.000e+000, 0.000e+000]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(q_vals,axis=1)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon$ case was at step 0 for sure since no previous reward could guide the selection of the best action.\n",
    "\n",
    "Step 2,3,4 and 5 where probably chosen randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Action-value implementation\n",
    "\n",
    "* incremental computation for stationary problems\n",
    "\n",
    "$$ Q_{n+1}\\ =\\ Q_n\\ +\\ \\frac{1}{n}\\bigr[R_n\\ -\\ Q_n\\bigl] $$\n",
    "\n",
    "* for non-stationary problems\n",
    "\n",
    "$$ Q_{n+1}\\ =\\ Q_n\\ +\\ \\alpha\\bigr[R_n\\ -\\ Q_n\\bigl] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Initial values\n",
    "\n",
    "Action values estimates $Q_0(a)$ can be zero or chosen higher than the actual rewards such that it encourage exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (graph)",
   "language": "python",
   "name": "graph_ml_problems"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
